# Exercise-13

Welcome to exercise-13. Here we will try to do some basics training about logging and debugging of our applications and components in Kubernetes environment.

## Kubernetes logs

Everything a containerized application write to **stdout** and **stderr** is handled and redirected somewhere by a container engine. For example, the Docker engine redirects those two streams to a logging driver (which is by default json-file).

To see the logs you need to run the below commands:

```bash
kubectl logs <name_of_the_pod> 

## for logs streaming
kubectl logs <name_of_the_pod> -f

## for selected container in multi container pod
kubectl logs <name_of_the_pod> -c <name_of_the_container_in_pod> 
```

Another option is to see the logs from the Kubernetes Dashboard. Go to kubernetes dashboard, select the pod then click on the logs icon to see the logs. From there you can easily switch between containers.

**kubetail** is another good tool to read the logs from multi containers to aggregate the view:

[https://github.com/johanhaleby/kubetail](https://github.com/johanhaleby/kubetail)

Installation and example usage:

```bash
sudo curl https://raw.githubusercontent.com/johanhaleby/kubetail/master/kubetail >> /usr/local/bin/kubetail
sudo chmod +x /usr/local/bin/kubetail

example of usage

kubetail <pod_prefix> -f
```

## EFK

EFK stands from Elasticsearch, Logstash and Kibana.

To install the components, follow the below steps.

1. Create the namespace for logging components

```bash
kubectl create namespace logging
```

2. **Elastisearch**

```bash
kubectl apply -f elastic.yaml -n logging

# verify

kubectl get pods -n logging
kubectl get svc -n logging

# take a note of NodePort of the elasticsearch service and use curl to access it
curl $(minikube ip):<port>
```

3. **Kibana**

```bash
kubectl apply -f kibana.yaml -n logging

# verify

kubectl get pods -n logging
kubectl get svc -n logging

# take a note of NodePort of the kibana service and access it using your browser:
http://MINIKUBE_IP:KIBANA_NODE_PORT
```

4. **Fluentd**

```bash
# Apply the necesssary RBAC permissions
kubectl create -f fluentd-rbac.yaml

# Create the DaemonSet
kubectl create -f fluetnd-daemonset.yaml

# Verify
kubectl get pod -n kube-system

# Take a note of fluentd pod and use here 
kubectl logs <fluentd_pod> -n kube-system

# Expect the output like:
# Connection opened to Elasticsearch cluster =>
#  {:host=>"elasticsearch.logging", :port=>9200, :scheme=>"http"}
```

5. Sample application

```
kubectl create -f app-example.yaml

kubectl get pod

# take a note of the pod and use now

kubectl logs -f <pod>

# It will print the timestamps every second
```

6. Configure Kibana
  - Click **Management** and then select **Index Patterns** under **Kibana**
  - Click the **Create index pattern**. Select the new Logstash index that is generated by the Fluentd DaemonSet ( **logstash + star**). Click **Next step**
  - Set the **Time Filter field name** to **@timestamp**. Then, click **Create index pattern**
  - Click **Discover** to view your application logs

